{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   TotalBsmtSFSqrt  OpenPorchSFLog  ...  Fireplaces_2  Fireplaces_3\n0        29.257478        4.110874  ...             0             0\n1        35.524639      -13.815511  ...             0             0\n2        30.331502        3.737670  ...             0             0\n4        33.837849        4.430817  ...             0             0\n5        28.213472        3.401197  ...             0             0\n\n[5 rows x 110 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TotalBsmtSFSqrt</th>\n      <th>OpenPorchSFLog</th>\n      <th>MasVnrAreaSqrt</th>\n      <th>YearRemodAdd</th>\n      <th>FirstFlrSFLog</th>\n      <th>GrLivAreaLog</th>\n      <th>LotAreaSqrt</th>\n      <th>YearBuilt</th>\n      <th>SalePriceLog</th>\n      <th>KitchenQual_Ex</th>\n      <th>KitchenQual_Fa</th>\n      <th>KitchenQual_Gd</th>\n      <th>KitchenQual_TA</th>\n      <th>OverallQual_1</th>\n      <th>OverallQual_2</th>\n      <th>OverallQual_3</th>\n      <th>OverallQual_4</th>\n      <th>OverallQual_5</th>\n      <th>OverallQual_6</th>\n      <th>OverallQual_7</th>\n      <th>OverallQual_8</th>\n      <th>OverallQual_9</th>\n      <th>OverallQual_10</th>\n      <th>Neighborhood_Blmngtn</th>\n      <th>Neighborhood_Blueste</th>\n      <th>Neighborhood_BrDale</th>\n      <th>Neighborhood_BrkSide</th>\n      <th>Neighborhood_ClearCr</th>\n      <th>Neighborhood_CollgCr</th>\n      <th>Neighborhood_Crawfor</th>\n      <th>Neighborhood_Edwards</th>\n      <th>Neighborhood_Gilbert</th>\n      <th>Neighborhood_IDOTRR</th>\n      <th>Neighborhood_MeadowV</th>\n      <th>Neighborhood_Mitchel</th>\n      <th>Neighborhood_NAmes</th>\n      <th>Neighborhood_NPkVill</th>\n      <th>Neighborhood_NWAmes</th>\n      <th>Neighborhood_NoRidge</th>\n      <th>Neighborhood_NridgHt</th>\n      <th>...</th>\n      <th>CentralAir_Y</th>\n      <th>Functional_Maj1</th>\n      <th>Functional_Maj2</th>\n      <th>Functional_Min1</th>\n      <th>Functional_Min2</th>\n      <th>Functional_Mod</th>\n      <th>Functional_Sev</th>\n      <th>Functional_Typ</th>\n      <th>GarageCars_0</th>\n      <th>GarageCars_1</th>\n      <th>GarageCars_2</th>\n      <th>GarageCars_3</th>\n      <th>GarageCars_4</th>\n      <th>MSSubClass_20</th>\n      <th>MSSubClass_30</th>\n      <th>MSSubClass_40</th>\n      <th>MSSubClass_45</th>\n      <th>MSSubClass_50</th>\n      <th>MSSubClass_60</th>\n      <th>MSSubClass_70</th>\n      <th>MSSubClass_75</th>\n      <th>MSSubClass_80</th>\n      <th>MSSubClass_85</th>\n      <th>MSSubClass_90</th>\n      <th>MSSubClass_120</th>\n      <th>MSSubClass_160</th>\n      <th>MSSubClass_180</th>\n      <th>MSSubClass_190</th>\n      <th>MSZoning_C (all)</th>\n      <th>MSZoning_FV</th>\n      <th>MSZoning_RH</th>\n      <th>MSZoning_RL</th>\n      <th>MSZoning_RM</th>\n      <th>PavedDrive_N</th>\n      <th>PavedDrive_P</th>\n      <th>PavedDrive_Y</th>\n      <th>Fireplaces_0</th>\n      <th>Fireplaces_1</th>\n      <th>Fireplaces_2</th>\n      <th>Fireplaces_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>29.257478</td>\n      <td>4.110874</td>\n      <td>14.000000</td>\n      <td>2003</td>\n      <td>6.752270</td>\n      <td>7.444249</td>\n      <td>91.923882</td>\n      <td>2003</td>\n      <td>12.247694</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35.524639</td>\n      <td>-13.815511</td>\n      <td>0.000000</td>\n      <td>1976</td>\n      <td>7.140453</td>\n      <td>7.140453</td>\n      <td>97.979590</td>\n      <td>1976</td>\n      <td>12.109011</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.331502</td>\n      <td>3.737670</td>\n      <td>12.727922</td>\n      <td>2002</td>\n      <td>6.824374</td>\n      <td>7.487734</td>\n      <td>106.066017</td>\n      <td>2001</td>\n      <td>12.317167</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33.837849</td>\n      <td>4.430817</td>\n      <td>18.708287</td>\n      <td>2000</td>\n      <td>7.043160</td>\n      <td>7.695303</td>\n      <td>119.415242</td>\n      <td>2000</td>\n      <td>12.429216</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>28.213472</td>\n      <td>3.401197</td>\n      <td>0.000000</td>\n      <td>1995</td>\n      <td>6.679599</td>\n      <td>7.216709</td>\n      <td>118.806565</td>\n      <td>1993</td>\n      <td>11.870600</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 110 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "loading_a_pickle_file = pickle.load(open(\"./DATA/house_df.pkl\", \"rb\"))\n",
    "loading_a_pickle_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1042, 110)\n(348, 110)\n"
    }
   ],
   "source": [
    "#Splitting train and test sets\n",
    "house_train, house_test = train_test_split(loading_a_pickle_file, test_size=0.25, random_state=42)\n",
    "print(house_train.shape)\n",
    "print(house_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split features from Label\n",
    "def split_features_labels(dataframe_name,label):\n",
    "    Y_ = pd.DataFrame(dataframe_name[[label]])\n",
    "    X_ = dataframe_name.drop([label], axis=1)\n",
    "    return(X_,Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the features from the Sale Price\n",
    "X_train, Y_train = split_features_labels(house_train,\"SalePriceLog\")\n",
    "X_test, Y_test = split_features_labels(house_test,\"SalePriceLog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "     SalePriceLog  SalePricePred\n582         118.5          129.8\n172         239.0          193.1\n248         180.0          203.4\n697         123.5          132.2\n281         185.0          183.7\n..            ...            ...\n972          99.5          142.1\n544         179.7          194.5\n477         380.0          351.8\n514          96.5          105.5\n343         266.0          312.3\n\n[348 rows x 2 columns]\nCorrelation :  95.54 %\n"
    }
   ],
   "source": [
    "#Predicting using RandomForest\n",
    "randomforestregressor = RandomForestRegressor(n_estimators = 100, max_features = 8,max_depth = 10)\n",
    "randomforestregressor.fit(X_train, Y_train)\n",
    "Y_prediction = randomforestregressor.predict(X_test)\n",
    "\n",
    "Y_test[\"SalePricePred\"] = Y_prediction\n",
    "print(\"{}\".format(round(np.exp(Y_test),-2)/1000))\n",
    "\n",
    "#Pred_df = Pred_df.rename(columns={0: \"SalePriceLog\"})\n",
    "\n",
    "print(\"Correlation : \", round((Y_test[\"SalePricePred\"].corr(Y_test[\"SalePriceLog\"]))*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average error is 9.26%\n"
    }
   ],
   "source": [
    "# Calculating the rate average rate of error(%)\n",
    "percentage_error = 0\n",
    "for _ , row in Y_test.iterrows():\n",
    "    percentage_error += np.abs(100*(np.exp(row[\"SalePricePred\"])-np.exp(row[\"SalePriceLog\"]))/np.exp(row[\"SalePriceLog\"]))\n",
    "print(\"Average error is {}%\".format(round(percentage_error/len(Y_test),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "     SalePriceLog  SalePricePred\n582         118.5          112.6\n172         239.0          204.9\n248         180.0          199.7\n697         123.5          117.9\n281         185.0          189.0\n..            ...            ...\n972          99.5          135.7\n544         179.7          182.0\n477         380.0          386.5\n514          96.5          112.3\n343         266.0          309.5\n\n[348 rows x 2 columns]\nCorrelation :  95.55 %\n"
    }
   ],
   "source": [
    "\n",
    "xg_reg = XGBRegressor( learning_rate = 0.1,\n",
    "                max_depth = 10, n_estimators = 100)\n",
    "\n",
    "xg_reg.fit(X_train,Y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "Y_test[\"SalePricePred\"] = preds\n",
    "print(\"{}\".format(round(np.exp(Y_test),-2)/1000))\n",
    "print(\"Correlation : \", round((Y_test[\"SalePricePred\"].corr(Y_test[\"SalePriceLog\"]))*100,2), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average error is 8.46%\n"
    }
   ],
   "source": [
    "# Calculating the rate average rate of error(%)\n",
    "percentage_error = 0\n",
    "for _ , row in Y_test.iterrows():\n",
    "    percentage_error += np.abs(100*(np.exp(row[\"SalePricePred\"])-np.exp(row[\"SalePriceLog\"]))/np.exp(row[\"SalePriceLog\"]))\n",
    "print(\"Average error is {}%\".format(round(percentage_error/len(Y_test),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ":\tlearn: 0.0028909\ttotal: 29m 30s\tremaining: 6m 18s\n1648:\tlearn: 0.0028909\ttotal: 29m 31s\tremaining: 6m 17s\n1649:\tlearn: 0.0028909\ttotal: 29m 32s\tremaining: 6m 16s\n1650:\tlearn: 0.0028909\ttotal: 29m 34s\tremaining: 6m 15s\n1651:\tlearn: 0.0028909\ttotal: 29m 35s\tremaining: 6m 13s\n1652:\tlearn: 0.0028909\ttotal: 29m 35s\tremaining: 6m 12s\n1653:\tlearn: 0.0028909\ttotal: 29m 36s\tremaining: 6m 11s\n1654:\tlearn: 0.0028909\ttotal: 29m 36s\tremaining: 6m 10s\n1655:\tlearn: 0.0028909\ttotal: 29m 37s\tremaining: 6m 9s\n1656:\tlearn: 0.0028909\ttotal: 29m 38s\tremaining: 6m 8s\n1657:\tlearn: 0.0028909\ttotal: 29m 39s\tremaining: 6m 7s\n1658:\tlearn: 0.0028909\ttotal: 29m 41s\tremaining: 6m 6s\n1659:\tlearn: 0.0028909\ttotal: 29m 42s\tremaining: 6m 5s\n1660:\tlearn: 0.0028909\ttotal: 29m 43s\tremaining: 6m 3s\n1661:\tlearn: 0.0028909\ttotal: 29m 44s\tremaining: 6m 2s\n1662:\tlearn: 0.0028909\ttotal: 29m 45s\tremaining: 6m 1s\n1663:\tlearn: 0.0028909\ttotal: 29m 46s\tremaining: 6m\n1664:\tlearn: 0.0028909\ttotal: 29m 47s\tremaining: 5m 59s\n1665:\tlearn: 0.0028909\ttotal: 29m 48s\tremaining: 5m 58s\n1666:\tlearn: 0.0028909\ttotal: 29m 49s\tremaining: 5m 57s\n1667:\tlearn: 0.0028909\ttotal: 29m 51s\tremaining: 5m 56s\n1668:\tlearn: 0.0028909\ttotal: 29m 52s\tremaining: 5m 55s\n1669:\tlearn: 0.0028909\ttotal: 29m 53s\tremaining: 5m 54s\n1670:\tlearn: 0.0028909\ttotal: 29m 54s\tremaining: 5m 53s\n1671:\tlearn: 0.0028909\ttotal: 29m 54s\tremaining: 5m 52s\n1672:\tlearn: 0.0028909\ttotal: 29m 55s\tremaining: 5m 50s\n1673:\tlearn: 0.0028909\ttotal: 29m 56s\tremaining: 5m 49s\n1674:\tlearn: 0.0028909\ttotal: 29m 57s\tremaining: 5m 48s\n1675:\tlearn: 0.0028909\ttotal: 29m 58s\tremaining: 5m 47s\n1676:\tlearn: 0.0028909\ttotal: 29m 59s\tremaining: 5m 46s\n1677:\tlearn: 0.0028909\ttotal: 30m 1s\tremaining: 5m 45s\n1678:\tlearn: 0.0028909\ttotal: 30m 2s\tremaining: 5m 44s\n1679:\tlearn: 0.0028909\ttotal: 30m 3s\tremaining: 5m 43s\n1680:\tlearn: 0.0028909\ttotal: 30m 5s\tremaining: 5m 42s\n1681:\tlearn: 0.0028909\ttotal: 30m 7s\tremaining: 5m 41s\n1682:\tlearn: 0.0028909\ttotal: 30m 8s\tremaining: 5m 40s\n1683:\tlearn: 0.0028909\ttotal: 30m 10s\tremaining: 5m 39s\n1684:\tlearn: 0.0028909\ttotal: 30m 12s\tremaining: 5m 38s\n1685:\tlearn: 0.0028909\ttotal: 30m 12s\tremaining: 5m 37s\n1686:\tlearn: 0.0028909\ttotal: 30m 14s\tremaining: 5m 36s\n1687:\tlearn: 0.0028909\ttotal: 30m 15s\tremaining: 5m 35s\n1688:\tlearn: 0.0028909\ttotal: 30m 17s\tremaining: 5m 34s\n1689:\tlearn: 0.0028909\ttotal: 30m 18s\tremaining: 5m 33s\n1690:\tlearn: 0.0028909\ttotal: 30m 19s\tremaining: 5m 32s\n1691:\tlearn: 0.0028909\ttotal: 30m 21s\tremaining: 5m 31s\n1692:\tlearn: 0.0028909\ttotal: 30m 22s\tremaining: 5m 30s\n1693:\tlearn: 0.0028909\ttotal: 30m 22s\tremaining: 5m 29s\n1694:\tlearn: 0.0028909\ttotal: 30m 24s\tremaining: 5m 28s\n1695:\tlearn: 0.0028909\ttotal: 30m 25s\tremaining: 5m 27s\n1696:\tlearn: 0.0028909\ttotal: 30m 25s\tremaining: 5m 25s\n1697:\tlearn: 0.0028909\ttotal: 30m 26s\tremaining: 5m 24s\n1698:\tlearn: 0.0028909\ttotal: 30m 27s\tremaining: 5m 23s\n1699:\tlearn: 0.0028909\ttotal: 30m 28s\tremaining: 5m 22s\n1700:\tlearn: 0.0028909\ttotal: 30m 30s\tremaining: 5m 21s\n1701:\tlearn: 0.0028909\ttotal: 30m 31s\tremaining: 5m 20s\n1702:\tlearn: 0.0028909\ttotal: 30m 32s\tremaining: 5m 19s\n1703:\tlearn: 0.0028909\ttotal: 30m 32s\tremaining: 5m 18s\n1704:\tlearn: 0.0028909\ttotal: 30m 33s\tremaining: 5m 17s\n1705:\tlearn: 0.0028909\ttotal: 30m 34s\tremaining: 5m 16s\n1706:\tlearn: 0.0028909\ttotal: 30m 36s\tremaining: 5m 15s\n1707:\tlearn: 0.0028909\ttotal: 30m 37s\tremaining: 5m 14s\n1708:\tlearn: 0.0028909\ttotal: 30m 38s\tremaining: 5m 13s\n1709:\tlearn: 0.0028909\ttotal: 30m 39s\tremaining: 5m 11s\n1710:\tlearn: 0.0028909\ttotal: 30m 40s\tremaining: 5m 10s\n1711:\tlearn: 0.0028909\ttotal: 30m 41s\tremaining: 5m 9s\n1712:\tlearn: 0.0028909\ttotal: 30m 43s\tremaining: 5m 8s\n1713:\tlearn: 0.0028909\ttotal: 30m 44s\tremaining: 5m 7s\n1714:\tlearn: 0.0028909\ttotal: 30m 45s\tremaining: 5m 6s\n1715:\tlearn: 0.0028909\ttotal: 30m 46s\tremaining: 5m 5s\n1716:\tlearn: 0.0028909\ttotal: 30m 47s\tremaining: 5m 4s\n1717:\tlearn: 0.0028909\ttotal: 30m 48s\tremaining: 5m 3s\n1718:\tlearn: 0.0028909\ttotal: 30m 49s\tremaining: 5m 2s\n1719:\tlearn: 0.0028909\ttotal: 30m 51s\tremaining: 5m 1s\n1720:\tlearn: 0.0028909\ttotal: 30m 52s\tremaining: 5m\n1721:\tlearn: 0.0028909\ttotal: 30m 53s\tremaining: 4m 59s\n1722:\tlearn: 0.0028909\ttotal: 30m 53s\tremaining: 4m 58s\n1723:\tlearn: 0.0028909\ttotal: 30m 55s\tremaining: 4m 56s\n1724:\tlearn: 0.0028909\ttotal: 30m 56s\tremaining: 4m 55s\n1725:\tlearn: 0.0028909\ttotal: 30m 56s\tremaining: 4m 54s\n1726:\tlearn: 0.0028909\ttotal: 30m 57s\tremaining: 4m 53s\n1727:\tlearn: 0.0028909\ttotal: 30m 58s\tremaining: 4m 52s\n1728:\tlearn: 0.0028909\ttotal: 30m 59s\tremaining: 4m 51s\n1729:\tlearn: 0.0028909\ttotal: 31m\tremaining: 4m 50s\n1730:\tlearn: 0.0028909\ttotal: 31m 2s\tremaining: 4m 49s\n1731:\tlearn: 0.0028909\ttotal: 31m 3s\tremaining: 4m 48s\n1732:\tlearn: 0.0028909\ttotal: 31m 4s\tremaining: 4m 47s\n1733:\tlearn: 0.0028909\ttotal: 31m 5s\tremaining: 4m 46s\n1734:\tlearn: 0.0028909\ttotal: 31m 6s\tremaining: 4m 45s\n1735:\tlearn: 0.0028909\ttotal: 31m 7s\tremaining: 4m 44s\n1736:\tlearn: 0.0028909\ttotal: 31m 8s\tremaining: 4m 42s\n1737:\tlearn: 0.0028909\ttotal: 31m 9s\tremaining: 4m 41s\n1738:\tlearn: 0.0028909\ttotal: 31m 10s\tremaining: 4m 40s\n1739:\tlearn: 0.0028909\ttotal: 31m 11s\tremaining: 4m 39s\n1740:\tlearn: 0.0028909\ttotal: 31m 13s\tremaining: 4m 38s\n1741:\tlearn: 0.0028909\ttotal: 31m 14s\tremaining: 4m 37s\n1742:\tlearn: 0.0028909\ttotal: 31m 15s\tremaining: 4m 36s\n1743:\tlearn: 0.0028909\ttotal: 31m 16s\tremaining: 4m 35s\n1744:\tlearn: 0.0028909\ttotal: 31m 17s\tremaining: 4m 34s\n1745:\tlearn: 0.0028909\ttotal: 31m 17s\tremaining: 4m 33s\n1746:\tlearn: 0.0028909\ttotal: 31m 19s\tremaining: 4m 32s\n1747:\tlearn: 0.0028909\ttotal: 31m 20s\tremaining: 4m 31s\n1748:\tlearn: 0.0028909\ttotal: 31m 21s\tremaining: 4m 30s\n1749:\tlearn: 0.0028909\ttotal: 31m 22s\tremaining: 4m 28s\n1750:\tlearn: 0.0028909\ttotal: 31m 23s\tremaining: 4m 27s\n1751:\tlearn: 0.0028909\ttotal: 31m 25s\tremaining: 4m 26s\n1752:\tlearn: 0.0028909\ttotal: 31m 26s\tremaining: 4m 25s\n1753:\tlearn: 0.0028909\ttotal: 31m 27s\tremaining: 4m 24s\n1754:\tlearn: 0.0028909\ttotal: 31m 28s\tremaining: 4m 23s\n1755:\tlearn: 0.0028909\ttotal: 31m 29s\tremaining: 4m 22s\n1756:\tlearn: 0.0028909\ttotal: 31m 30s\tremaining: 4m 21s\n1757:\tlearn: 0.0028909\ttotal: 31m 31s\tremaining: 4m 20s\n1758:\tlearn: 0.0028909\ttotal: 31m 32s\tremaining: 4m 19s\n1759:\tlearn: 0.0028909\ttotal: 31m 34s\tremaining: 4m 18s\n1760:\tlearn: 0.0028909\ttotal: 31m 35s\tremaining: 4m 17s\n1761:\tlearn: 0.0028909\ttotal: 31m 36s\tremaining: 4m 16s\n1762:\tlearn: 0.0028909\ttotal: 31m 37s\tremaining: 4m 15s\n1763:\tlearn: 0.0028909\ttotal: 31m 38s\tremaining: 4m 14s\n1764:\tlearn: 0.0028909\ttotal: 31m 39s\tremaining: 4m 12s\n1765:\tlearn: 0.0028909\ttotal: 31m 40s\tremaining: 4m 11s\n1766:\tlearn: 0.0028909\ttotal: 31m 41s\tremaining: 4m 10s\n1767:\tlearn: 0.0028909\ttotal: 31m 43s\tremaining: 4m 9s\n1768:\tlearn: 0.0028909\ttotal: 31m 44s\tremaining: 4m 8s\n1769:\tlearn: 0.0028909\ttotal: 31m 45s\tremaining: 4m 7s\n1770:\tlearn: 0.0028909\ttotal: 31m 46s\tremaining: 4m 6s\n1771:\tlearn: 0.0028909\ttotal: 31m 47s\tremaining: 4m 5s\n1772:\tlearn: 0.0028909\ttotal: 31m 48s\tremaining: 4m 4s\n1773:\tlearn: 0.0028909\ttotal: 31m 49s\tremaining: 4m 3s\n1774:\tlearn: 0.0028909\ttotal: 31m 50s\tremaining: 4m 2s\n1775:\tlearn: 0.0028909\ttotal: 31m 52s\tremaining: 4m 1s\n1776:\tlearn: 0.0028909\ttotal: 31m 53s\tremaining: 4m\n1777:\tlearn: 0.0028909\ttotal: 31m 54s\tremaining: 3m 59s\n1778:\tlearn: 0.0028909\ttotal: 31m 55s\tremaining: 3m 57s\n1779:\tlearn: 0.0028909\ttotal: 31m 56s\tremaining: 3m 56s\n1780:\tlearn: 0.0028909\ttotal: 31m 57s\tremaining: 3m 55s\n1781:\tlearn: 0.0028909\ttotal: 31m 58s\tremaining: 3m 54s\n1782:\tlearn: 0.0028909\ttotal: 31m 59s\tremaining: 3m 53s\n1783:\tlearn: 0.0028909\ttotal: 32m\tremaining: 3m 52s\n1784:\tlearn: 0.0028909\ttotal: 32m 2s\tremaining: 3m 51s\n1785:\tlearn: 0.0028909\ttotal: 32m 3s\tremaining: 3m 50s\n1786:\tlearn: 0.0028909\ttotal: 32m 4s\tremaining: 3m 49s\n1787:\tlearn: 0.0028909\ttotal: 32m 5s\tremaining: 3m 48s\n1788:\tlearn: 0.0028909\ttotal: 32m 6s\tremaining: 3m 47s\n1789:\tlearn: 0.0028909\ttotal: 32m 7s\tremaining: 3m 46s\n1790:\tlearn: 0.0028909\ttotal: 32m 8s\tremaining: 3m 45s\n1791:\tlearn: 0.0028909\ttotal: 32m 10s\tremaining: 3m 44s\n1792:\tlearn: 0.0028909\ttotal: 32m 11s\tremaining: 3m 42s\n1793:\tlearn: 0.0028909\ttotal: 32m 12s\tremaining: 3m 41s\n1794:\tlearn: 0.0028909\ttotal: 32m 13s\tremaining: 3m 40s\n1795:\tlearn: 0.0028909\ttotal: 32m 14s\tremaining: 3m 39s\n1796:\tlearn: 0.0028909\ttotal: 32m 15s\tremaining: 3m 38s\n1797:\tlearn: 0.0028909\ttotal: 32m 16s\tremaining: 3m 37s\n1798:\tlearn: 0.0028909\ttotal: 32m 17s\tremaining: 3m 36s\n1799:\tlearn: 0.0028909\ttotal: 32m 18s\tremaining: 3m 35s\n1800:\tlearn: 0.0028909\ttotal: 32m 20s\tremaining: 3m 34s\n1801:\tlearn: 0.0028909\ttotal: 32m 21s\tremaining: 3m 33s\n1802:\tlearn: 0.0028909\ttotal: 32m 22s\tremaining: 3m 32s\n1803:\tlearn: 0.0028909\ttotal: 32m 23s\tremaining: 3m 31s\n1804:\tlearn: 0.0028909\ttotal: 32m 24s\tremaining: 3m 30s\n1805:\tlearn: 0.0028909\ttotal: 32m 25s\tremaining: 3m 29s\n1806:\tlearn: 0.0028909\ttotal: 32m 26s\tremaining: 3m 27s\n1807:\tlearn: 0.0028909\ttotal: 32m 28s\tremaining: 3m 26s\n1808:\tlearn: 0.0028909\ttotal: 32m 29s\tremaining: 3m 25s\n1809:\tlearn: 0.0028909\ttotal: 32m 30s\tremaining: 3m 24s\n1810:\tlearn: 0.0028909\ttotal: 32m 31s\tremaining: 3m 23s\n1811:\tlearn: 0.0028909\ttotal: 32m 32s\tremaining: 3m 22s\n1812:\tlearn: 0.0028909\ttotal: 32m 33s\tremaining: 3m 21s\n1813:\tlearn: 0.0028909\ttotal: 32m 34s\tremaining: 3m 20s\n1814:\tlearn: 0.0028909\ttotal: 32m 36s\tremaining: 3m 19s\n1815:\tlearn: 0.0028909\ttotal: 32m 37s\tremaining: 3m 18s\n1816:\tlearn: 0.0028909\ttotal: 32m 38s\tremaining: 3m 17s\n1817:\tlearn: 0.0028909\ttotal: 32m 39s\tremaining: 3m 16s\n1818:\tlearn: 0.0028909\ttotal: 32m 40s\tremaining: 3m 15s\n1819:\tlearn: 0.0028909\ttotal: 32m 42s\tremaining: 3m 14s\n1820:\tlearn: 0.0028909\ttotal: 32m 43s\tremaining: 3m 13s\n1821:\tlearn: 0.0028909\ttotal: 32m 44s\tremaining: 3m 11s\n1822:\tlearn: 0.0028909\ttotal: 32m 45s\tremaining: 3m 10s\n1823:\tlearn: 0.0028909\ttotal: 32m 46s\tremaining: 3m 9s\n1824:\tlearn: 0.0028909\ttotal: 32m 47s\tremaining: 3m 8s\n1825:\tlearn: 0.0028909\ttotal: 32m 49s\tremaining: 3m 7s\n1826:\tlearn: 0.0028909\ttotal: 32m 50s\tremaining: 3m 6s\n1827:\tlearn: 0.0028909\ttotal: 32m 51s\tremaining: 3m 5s\n1828:\tlearn: 0.0028909\ttotal: 32m 52s\tremaining: 3m 4s\n1829:\tlearn: 0.0028909\ttotal: 32m 53s\tremaining: 3m 3s\n1830:\tlearn: 0.0028909\ttotal: 32m 54s\tremaining: 3m 2s\n1831:\tlearn: 0.0028909\ttotal: 32m 55s\tremaining: 3m 1s\n1832:\tlearn: 0.0028909\ttotal: 32m 56s\tremaining: 3m\n1833:\tlearn: 0.0028909\ttotal: 32m 57s\tremaining: 2m 58s\n1834:\tlearn: 0.0028909\ttotal: 32m 58s\tremaining: 2m 57s\n1835:\tlearn: 0.0028909\ttotal: 32m 59s\tremaining: 2m 56s\n1836:\tlearn: 0.0028909\ttotal: 33m\tremaining: 2m 55s\n1837:\tlearn: 0.0028909\ttotal: 33m 2s\tremaining: 2m 54s\n1838:\tlearn: 0.0028909\ttotal: 33m 3s\tremaining: 2m 53s\n1839:\tlearn: 0.0028909\ttotal: 33m 4s\tremaining: 2m 52s\n1840:\tlearn: 0.0028909\ttotal: 33m 5s\tremaining: 2m 51s\n1841:\tlearn: 0.0028909\ttotal: 33m 6s\tremaining: 2m 50s\n1842:\tlearn: 0.0028909\ttotal: 33m 7s\tremaining: 2m 49s\n1843:\tlearn: 0.0028909\ttotal: 33m 8s\tremaining: 2m 48s\n1844:\tlearn: 0.0028909\ttotal: 33m 10s\tremaining: 2m 47s\n1845:\tlearn: 0.0028909\ttotal: 33m 11s\tremaining: 2m 46s\n1846:\tlearn: 0.0028909\ttotal: 33m 12s\tremaining: 2m 45s\n1847:\tlearn: 0.0028909\ttotal: 33m 13s\tremaining: 2m 43s\n1848:\tlearn: 0.0028909\ttotal: 33m 14s\tremaining: 2m 42s\n1849:\tlearn: 0.0028909\ttotal: 33m 15s\tremaining: 2m 41s\n1850:\tlearn: 0.0028909\ttotal: 33m 16s\tremaining: 2m 40s\n1851:\tlearn: 0.0028909\ttotal: 33m 17s\tremaining: 2m 39s\n1852:\tlearn: 0.0028909\ttotal: 33m 19s\tremaining: 2m 38s\n1853:\tlearn: 0.0028909\ttotal: 33m 20s\tremaining: 2m 37s\n1854:\tlearn: 0.0028909\ttotal: 33m 21s\tremaining: 2m 36s\n1855:\tlearn: 0.0028909\ttotal: 33m 22s\tremaining: 2m 35s\n1856:\tlearn: 0.0028909\ttotal: 33m 23s\tremaining: 2m 34s\n1857:\tlearn: 0.0028909\ttotal: 33m 24s\tremaining: 2m 33s\n1858:\tlearn: 0.0028909\ttotal: 33m 25s\tremaining: 2m 32s\n1859:\tlearn: 0.0028909\ttotal: 33m 27s\tremaining: 2m 31s\n1860:\tlearn: 0.0028909\ttotal: 33m 28s\tremaining: 2m 29s\n1861:\tlearn: 0.0028909\ttotal: 33m 29s\tremaining: 2m 28s\n1862:\tlearn: 0.0028909\ttotal: 33m 30s\tremaining: 2m 27s\n1863:\tlearn: 0.0028909\ttotal: 33m 31s\tremaining: 2m 26s\n1864:\tlearn: 0.0028909\ttotal: 33m 32s\tremaining: 2m 25s\n1865:\tlearn: 0.0028909\ttotal: 33m 33s\tremaining: 2m 24s\n1866:\tlearn: 0.0028909\ttotal: 33m 35s\tremaining: 2m 23s\n1867:\tlearn: 0.0028909\ttotal: 33m 36s\tremaining: 2m 22s\n1868:\tlearn: 0.0028909\ttotal: 33m 37s\tremaining: 2m 21s\n1869:\tlearn: 0.0028909\ttotal: 33m 38s\tremaining: 2m 20s\n1870:\tlearn: 0.0028909\ttotal: 33m 39s\tremaining: 2m 19s\n1871:\tlearn: 0.0028909\ttotal: 33m 40s\tremaining: 2m 18s\n1872:\tlearn: 0.0028909\ttotal: 33m 41s\tremaining: 2m 17s\n1873:\tlearn: 0.0028909\ttotal: 33m 41s\tremaining: 2m 15s\n1874:\tlearn: 0.0028909\ttotal: 33m 42s\tremaining: 2m 14s\n1875:\tlearn: 0.0028909\ttotal: 33m 44s\tremaining: 2m 13s\n1876:\tlearn: 0.0028909\ttotal: 33m 45s\tremaining: 2m 12s\n1877:\tlearn: 0.0028909\ttotal: 33m 46s\tremaining: 2m 11s\n1878:\tlearn: 0.0028909\ttotal: 33m 47s\tremaining: 2m 10s\n1879:\tlearn: 0.0028909\ttotal: 33m 48s\tremaining: 2m 9s\n1880:\tlearn: 0.0028909\ttotal: 33m 49s\tremaining: 2m 8s\n1881:\tlearn: 0.0028909\ttotal: 33m 50s\tremaining: 2m 7s\n1882:\tlearn: 0.0028909\ttotal: 33m 50s\tremaining: 2m 6s\n1883:\tlearn: 0.0028909\ttotal: 33m 51s\tremaining: 2m 5s\n1884:\tlearn: 0.0028909\ttotal: 33m 52s\tremaining: 2m 4s\n1885:\tlearn: 0.0028909\ttotal: 33m 54s\tremaining: 2m 2s\n1886:\tlearn: 0.0028909\ttotal: 33m 55s\tremaining: 2m 1s\n1887:\tlearn: 0.0028909\ttotal: 33m 56s\tremaining: 2m\n1888:\tlearn: 0.0028909\ttotal: 33m 57s\tremaining: 1m 59s\n1889:\tlearn: 0.0028909\ttotal: 33m 58s\tremaining: 1m 58s\n1890:\tlearn: 0.0028909\ttotal: 33m 59s\tremaining: 1m 57s\n1891:\tlearn: 0.0028909\ttotal: 34m\tremaining: 1m 56s\n1892:\tlearn: 0.0028909\ttotal: 34m 1s\tremaining: 1m 55s\n1893:\tlearn: 0.0028909\ttotal: 34m 3s\tremaining: 1m 54s\n1894:\tlearn: 0.0028909\ttotal: 34m 3s\tremaining: 1m 53s\n1895:\tlearn: 0.0028909\ttotal: 34m 4s\tremaining: 1m 52s\n1896:\tlearn: 0.0028909\ttotal: 34m 5s\tremaining: 1m 51s\n1897:\tlearn: 0.0028909\ttotal: 34m 6s\tremaining: 1m 49s\n1898:\tlearn: 0.0028909\ttotal: 34m 7s\tremaining: 1m 48s\n1899:\tlearn: 0.0028909\ttotal: 34m 9s\tremaining: 1m 47s\n1900:\tlearn: 0.0028909\ttotal: 34m 10s\tremaining: 1m 46s\n1901:\tlearn: 0.0028909\ttotal: 34m 11s\tremaining: 1m 45s\n1902:\tlearn: 0.0028909\ttotal: 34m 12s\tremaining: 1m 44s\n1903:\tlearn: 0.0028909\ttotal: 34m 13s\tremaining: 1m 43s\n1904:\tlearn: 0.0028909\ttotal: 34m 14s\tremaining: 1m 42s\n1905:\tlearn: 0.0028909\ttotal: 34m 15s\tremaining: 1m 41s\n1906:\tlearn: 0.0028909\ttotal: 34m 16s\tremaining: 1m 40s\n1907:\tlearn: 0.0028909\ttotal: 34m 18s\tremaining: 1m 39s\n1908:\tlearn: 0.0028909\ttotal: 34m 19s\tremaining: 1m 38s\n1909:\tlearn: 0.0028909\ttotal: 34m 20s\tremaining: 1m 37s\n1910:\tlearn: 0.0028909\ttotal: 34m 21s\tremaining: 1m 36s\n1911:\tlearn: 0.0028909\ttotal: 34m 22s\tremaining: 1m 34s\n1912:\tlearn: 0.0028909\ttotal: 34m 23s\tremaining: 1m 33s\n1913:\tlearn: 0.0028909\ttotal: 34m 25s\tremaining: 1m 32s\n1914:\tlearn: 0.0028909\ttotal: 34m 26s\tremaining: 1m 31s\n1915:\tlearn: 0.0028909\ttotal: 34m 27s\tremaining: 1m 30s\n1916:\tlearn: 0.0028909\ttotal: 34m 28s\tremaining: 1m 29s\n1917:\tlearn: 0.0028909\ttotal: 34m 29s\tremaining: 1m 28s\n1918:\tlearn: 0.0028909\ttotal: 34m 31s\tremaining: 1m 27s\n1919:\tlearn: 0.0028909\ttotal: 34m 32s\tremaining: 1m 26s\n1920:\tlearn: 0.0028909\ttotal: 34m 33s\tremaining: 1m 25s\n1921:\tlearn: 0.0028909\ttotal: 34m 34s\tremaining: 1m 24s\n1922:\tlearn: 0.0028909\ttotal: 34m 34s\tremaining: 1m 23s\n1923:\tlearn: 0.0028909\ttotal: 34m 35s\tremaining: 1m 21s\n1924:\tlearn: 0.0028909\ttotal: 34m 36s\tremaining: 1m 20s\n1925:\tlearn: 0.0028909\ttotal: 34m 37s\tremaining: 1m 19s\n1926:\tlearn: 0.0028909\ttotal: 34m 38s\tremaining: 1m 18s\n1927:\tlearn: 0.0028909\ttotal: 34m 39s\tremaining: 1m 17s\n1928:\tlearn: 0.0028909\ttotal: 34m 40s\tremaining: 1m 16s\n1929:\tlearn: 0.0028909\ttotal: 34m 42s\tremaining: 1m 15s\n1930:\tlearn: 0.0028909\ttotal: 34m 43s\tremaining: 1m 14s\n1931:\tlearn: 0.0028909\ttotal: 34m 44s\tremaining: 1m 13s\n1932:\tlearn: 0.0028909\ttotal: 34m 45s\tremaining: 1m 12s\n1933:\tlearn: 0.0028909\ttotal: 34m 46s\tremaining: 1m 11s\n1934:\tlearn: 0.0028909\ttotal: 34m 47s\tremaining: 1m 10s\n1935:\tlearn: 0.0028909\ttotal: 34m 48s\tremaining: 1m 9s\n1936:\tlearn: 0.0028909\ttotal: 34m 49s\tremaining: 1m 7s\n1937:\tlearn: 0.0028909\ttotal: 34m 51s\tremaining: 1m 6s\n1938:\tlearn: 0.0028909\ttotal: 34m 52s\tremaining: 1m 5s\n1939:\tlearn: 0.0028909\ttotal: 34m 53s\tremaining: 1m 4s\n1940:\tlearn: 0.0028909\ttotal: 34m 54s\tremaining: 1m 3s\n1941:\tlearn: 0.0028909\ttotal: 34m 55s\tremaining: 1m 2s\n1942:\tlearn: 0.0028909\ttotal: 34m 56s\tremaining: 1m 1s\n1943:\tlearn: 0.0028909\ttotal: 34m 56s\tremaining: 1m\n1944:\tlearn: 0.0028909\ttotal: 34m 57s\tremaining: 59.3s\n1945:\tlearn: 0.0028909\ttotal: 34m 59s\tremaining: 58.2s\n1946:\tlearn: 0.0028909\ttotal: 35m\tremaining: 57.2s\n1947:\tlearn: 0.0028909\ttotal: 35m 1s\tremaining: 56.1s\n1948:\tlearn: 0.0028909\ttotal: 35m 2s\tremaining: 55s\n1949:\tlearn: 0.0028909\ttotal: 35m 3s\tremaining: 53.9s\n1950:\tlearn: 0.0028909\ttotal: 35m 4s\tremaining: 52.9s\n1951:\tlearn: 0.0028909\ttotal: 35m 5s\tremaining: 51.8s\n1952:\tlearn: 0.0028909\ttotal: 35m 6s\tremaining: 50.7s\n1953:\tlearn: 0.0028909\ttotal: 35m 8s\tremaining: 49.6s\n1954:\tlearn: 0.0028909\ttotal: 35m 9s\tremaining: 48.5s\n1955:\tlearn: 0.0028909\ttotal: 35m 9s\tremaining: 47.4s\n1956:\tlearn: 0.0028909\ttotal: 35m 10s\tremaining: 46.4s\n1957:\tlearn: 0.0028909\ttotal: 35m 11s\tremaining: 45.3s\n1958:\tlearn: 0.0028909\ttotal: 35m 12s\tremaining: 44.2s\n1959:\tlearn: 0.0028909\ttotal: 35m 13s\tremaining: 43.1s\n1960:\tlearn: 0.0028909\ttotal: 35m 14s\tremaining: 42.1s\n1961:\tlearn: 0.0028909\ttotal: 35m 15s\tremaining: 41s\n1962:\tlearn: 0.0028909\ttotal: 35m 17s\tremaining: 39.9s\n1963:\tlearn: 0.0028909\ttotal: 35m 17s\tremaining: 38.8s\n1964:\tlearn: 0.0028909\ttotal: 35m 17s\tremaining: 37.7s\n1965:\tlearn: 0.0028909\ttotal: 35m 18s\tremaining: 36.6s\n1966:\tlearn: 0.0028909\ttotal: 35m 19s\tremaining: 35.6s\n1967:\tlearn: 0.0028909\ttotal: 35m 20s\tremaining: 34.5s\n1968:\tlearn: 0.0028909\ttotal: 35m 21s\tremaining: 33.4s\n1969:\tlearn: 0.0028909\ttotal: 35m 22s\tremaining: 32.3s\n1970:\tlearn: 0.0028909\ttotal: 35m 23s\tremaining: 31.2s\n1971:\tlearn: 0.0028909\ttotal: 35m 24s\tremaining: 30.2s\n1972:\tlearn: 0.0028909\ttotal: 35m 26s\tremaining: 29.1s\n1973:\tlearn: 0.0028909\ttotal: 35m 27s\tremaining: 28s\n1974:\tlearn: 0.0028909\ttotal: 35m 28s\tremaining: 26.9s\n1975:\tlearn: 0.0028909\ttotal: 35m 29s\tremaining: 25.9s\n1976:\tlearn: 0.0028909\ttotal: 35m 30s\tremaining: 24.8s\n1977:\tlearn: 0.0028909\ttotal: 35m 31s\tremaining: 23.7s\n1978:\tlearn: 0.0028909\ttotal: 35m 32s\tremaining: 22.6s\n1979:\tlearn: 0.0028909\ttotal: 35m 34s\tremaining: 21.6s\n1980:\tlearn: 0.0028909\ttotal: 35m 35s\tremaining: 20.5s\n1981:\tlearn: 0.0028909\ttotal: 35m 35s\tremaining: 19.4s\n1982:\tlearn: 0.0028909\ttotal: 35m 36s\tremaining: 18.3s\n1983:\tlearn: 0.0028909\ttotal: 35m 37s\tremaining: 17.2s\n1984:\tlearn: 0.0028909\ttotal: 35m 38s\tremaining: 16.2s\n1985:\tlearn: 0.0028909\ttotal: 35m 39s\tremaining: 15.1s\n1986:\tlearn: 0.0028909\ttotal: 35m 41s\tremaining: 14s\n1987:\tlearn: 0.0028909\ttotal: 35m 42s\tremaining: 12.9s\n1988:\tlearn: 0.0028909\ttotal: 35m 43s\tremaining: 11.9s\n1989:\tlearn: 0.0028909\ttotal: 35m 44s\tremaining: 10.8s\n1990:\tlearn: 0.0028909\ttotal: 35m 45s\tremaining: 9.7s\n1991:\tlearn: 0.0028909\ttotal: 35m 46s\tremaining: 8.62s\n1992:\tlearn: 0.0028909\ttotal: 35m 47s\tremaining: 7.54s\n1993:\tlearn: 0.0028909\ttotal: 35m 48s\tremaining: 6.47s\n1994:\tlearn: 0.0028909\ttotal: 35m 49s\tremaining: 5.39s\n1995:\tlearn: 0.0028909\ttotal: 35m 50s\tremaining: 4.31s\n1996:\tlearn: 0.0028909\ttotal: 35m 51s\tremaining: 3.23s\n1997:\tlearn: 0.0028909\ttotal: 35m 52s\tremaining: 2.15s\n1998:\tlearn: 0.0028909\ttotal: 35m 53s\tremaining: 1.08s\n1999:\tlearn: 0.0028909\ttotal: 35m 54s\tremaining: 0us\n     SalePriceLog  SalePricePred\n582         118.5          135.8\n172         239.0          192.7\n248         180.0          197.7\n697         123.5          132.3\n281         185.0          192.7\n..            ...            ...\n972          99.5          143.3\n544         179.7          182.1\n477         380.0          380.0\n514          96.5          106.2\n343         266.0          290.5\n\n[348 rows x 2 columns]\nCorrelation :  94.74 %\n"
    }
   ],
   "source": [
    "\n",
    "ct_reg = CatBoostRegressor( learning_rate = 0.1,\n",
    "                max_depth = 10, n_estimators = 1000)\n",
    "\n",
    "ct_reg.fit(X_train,Y_train)\n",
    "preds = ct_reg.predict(X_test)\n",
    "Y_test[\"SalePricePred\"] = preds\n",
    "print(\"{}\".format(round(np.exp(Y_test),-2)/1000))\n",
    "print(\"Correlation : \", round((Y_test[\"SalePricePred\"].corr(Y_test[\"SalePriceLog\"]))*100,2), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average error is 8.19%\n"
    }
   ],
   "source": [
    "# Calculating the rate average rate of error(%)\n",
    "percentage_error = 0\n",
    "for _ , row in Y_test.iterrows():\n",
    "    percentage_error += np.abs(100*(np.exp(row[\"SalePricePred\"])-np.exp(row[\"SalePriceLog\"]))/np.exp(row[\"SalePriceLog\"]))\n",
    "print(\"Average error is {}%\".format(round(percentage_error/len(Y_test),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n     SalePriceLog  SalePricePred\n582         118.5          126.6\n172         239.0          200.5\n248         180.0          199.0\n697         123.5          125.1\n281         185.0          194.2\n..            ...            ...\n972          99.5          138.2\n544         179.7          173.8\n477         380.0          394.1\n514          96.5           98.7\n343         266.0          288.9\n\n[348 rows x 2 columns]\nCorrelation :  95.59 %\n"
    }
   ],
   "source": [
    "lg_reg = LGBMRegressor( learning_rate = 0.05,\n",
    "                max_depth = 10, n_estimators = 1000)\n",
    "\n",
    "lg_reg.fit(X_train,Y_train)\n",
    "preds = lg_reg.predict(X_test)\n",
    "Y_test[\"SalePricePred\"] = preds\n",
    "print(\"{}\".format(round(np.exp(Y_test),-2)/1000))\n",
    "print(\"Correlation : \", round((Y_test[\"SalePricePred\"].corr(Y_test[\"SalePriceLog\"]))*100,2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average error is 8.6%\n"
    }
   ],
   "source": [
    "# Calculating the rate average rate of error(%)\n",
    "percentage_error = 0\n",
    "for _ , row in Y_test.iterrows():\n",
    "    percentage_error += np.abs(100*(np.exp(row[\"SalePricePred\"])-np.exp(row[\"SalePriceLog\"]))/np.exp(row[\"SalePriceLog\"]))\n",
    "print(\"Average error is {}%\".format(round(percentage_error/len(Y_test),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-candidate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}